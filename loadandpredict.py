# -*- coding: utf-8 -*-
"""loadAndPredict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Est40PqMhABquh4x7-JF4WzcEJYK2K6
"""
import tokenization
from tensorflow.keras.models import load_model
import tensorflow_hub as hub
import numpy as np
import pandas as pd

# print("Loading Model in TF")

model = load_model('./model_1.h5', custom_objects={'KerasLayer': hub.KerasLayer})

# print("Loaded Model in TF")

# print("Building DATALOADER")

class DisasterDetectorLoader:
    
    def __init__(self, model, bert_layer, max_seq_length=128, lr=0.0001, epochs=15, batch_size=32):
        
        # BERT and Tokenization params
        self.bert_layer = bert_layer
        
        self.max_seq_length = max_seq_length        
        vocab_file = self.bert_layer.resolved_object.vocab_file.asset_path.numpy()
        do_lower_case = self.bert_layer.resolved_object.do_lower_case.numpy()
        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)
        
        # Learning control params
        self.lr = lr
        self.epochs = epochs
        self.batch_size = batch_size
        
        self.models = [model]
        self.scores = {}
        
        
    def encode(self, texts):
                
        all_tokens = []
        all_masks = []
        all_segments = []

        for text in texts:
            text = self.tokenizer.tokenize(text)
            text = text[:self.max_seq_length - 2]
            input_sequence = ['[CLS]'] + text + ['[SEP]']
            pad_len = self.max_seq_length - len(input_sequence)

            tokens = self.tokenizer.convert_tokens_to_ids(input_sequence)
            tokens += [0] * pad_len
            pad_masks = [1] * len(input_sequence) + [0] * pad_len
            segment_ids = [0] * self.max_seq_length

            all_tokens.append(tokens)
            all_masks.append(pad_masks)
            all_segments.append(segment_ids)

        return np.array(all_tokens), np.array(all_masks), np.array(all_segments)
        
        
    def predict(self, X):
        
        X_test_encoded = self.encode(X['text_cleaned'].str.lower())
        y_pred = np.zeros((X_test_encoded[0].shape[0], 1))

        for model in self.models:
            y_pred += model.predict(X_test_encoded) / len(self.models)

        return y_pred

# print("Built DATALOADER")

# print("Analysing", text)
# print("CALLING BERT LAYER")

bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1', trainable=True)

# print("CALLED BERT LAYER")

# print("Loading Model in Dataloader")

loadedCLF = DisasterDetectorLoader(model, bert_layer)

from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/square', methods=['POST'])
def square():
    text = str(request.json['input'])
    print(text)
    y_test = pd.DataFrame([{'text_cleaned': text}])
    output_number = loadedCLF.predict(y_test)
    print("OUTPUT", output_number)
    return jsonify({'output': output_number[0][0]})

if __name__ == '__main__':
    app.run(port=8888)